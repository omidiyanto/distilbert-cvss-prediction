{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS_Prediction\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\O.Midiyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\O.Midiyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\O.Midiyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\O.Midiyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\O.Midiyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import csv\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def read_cvss_txt(split_dir, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a directory structure and returns texts and labels.\n",
    "    Assumes directories named with class labels (e.g., LOW, HIGH).\n",
    "    \"\"\"\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"LOW\", \"HIGH\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == label_dir:\n",
    "                    labels.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def read_cvss_csv(file_name, num_label, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing texts and labels, and returns the texts and corresponding integer labels.\n",
    "    This function handles UTF-8 encoding to avoid issues with non-ASCII characters.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # Use 'with open' to ensure the file is properly closed after reading\n",
    "    with open(file_name, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "        \n",
    "        # Skip header row if it exists\n",
    "        next(csv_reader, None)  # This will skip the header, if present\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            texts.append(row[0])  # Assuming the first column is the text\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == row[num_label]:  # Match the label with classes\n",
    "                    labels.append(i)\n",
    "                    break  # Exit the loop once a match is found\n",
    "\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokenizer_model(model_name, extra_tokens, token_file, num_labels):\n",
    "    global lemmatization\n",
    "\n",
    "    print(\"### Selecting Model and Tokenizer\")\n",
    "\n",
    "    if model_name == 'distilbert':\n",
    "        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
    "        config = DistilBertConfig.from_pretrained('distilbert-base-cased')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "        model = DistilBertForSequenceClassification(config)\n",
    "    \n",
    "    elif model_name == 'bert':\n",
    "        from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'deberta':\n",
    "        from transformers import DebertaConfig, DebertaTokenizerFast, DebertaForSequenceClassification\n",
    "        config = DebertaConfig.from_pretrained('microsoft/deberta-base')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
    "        model = DebertaForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'albert':\n",
    "        from transformers import AlbertConfig, AlbertTokenizerFast, AlbertForSequenceClassification\n",
    "        config = AlbertConfig.from_pretrained('albert-base-v1')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1')\n",
    "        model = AlbertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'roberta':\n",
    "        from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "        config = RobertaConfig.from_pretrained('roberta-base')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "\n",
    "    ### Add Tokens\n",
    "    if extra_tokens:\n",
    "        add_tokens_from_file(token_file, tokenizer, lemmatization)\n",
    "    number_tokens = len(tokenizer)\n",
    "\n",
    "    print(\"### Number of tokens in Tokenizer\")\n",
    "    print(number_tokens)\n",
    "\n",
    "    # print(\"### Configuration\")\n",
    "    # print(model.config)\n",
    "\n",
    "    model.resize_token_embeddings(number_tokens) \n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def add_tokens_from_file(token_file, tokenizer, lemmatize=False):\n",
    "    print(\"### Adding Tokens\")\n",
    "    \n",
    "    file_      = open(token_file, 'r')\n",
    "    token_list = []\n",
    "    \n",
    "    for line in file_:\n",
    "        if lemmatize:\n",
    "            token_list.append(lemmatize_noun(line.rstrip(\"\\n\")))\n",
    "        else:\n",
    "            token_list.append(line.rstrip(\"\\n\"))\n",
    "    file_.close()\n",
    "    tokenizer.add_tokens(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    word_list = word_tokenize(sentence)\n",
    "    # lemmatized_output = ' '.join([lemmatize_word(w) for w in word_list]) # ALL LEMMATIZATION\n",
    "    lemmatized_output = ' '.join([lemmatize_noun(w) for w in word_list]) # NOUN LEMMATIZATION (OLD)\n",
    "\n",
    "    return lemmatized_output\n",
    "\n",
    "def lemmatize(train_texts, test_texts=None):\n",
    "    ### Lemmatize Sentences\n",
    "    lemmatized_texts_train = []\n",
    "    lemmatized_texts_test  = []\n",
    "    for text in train_texts:\n",
    "        lemmatized_texts_train.append(lemmatize_sentence(text))\n",
    "    if test_texts is not None:\n",
    "        for text in test_texts:\n",
    "            lemmatized_texts_test.append(lemmatize_sentence(text))\n",
    "\n",
    "    return lemmatized_texts_train, lemmatized_texts_test\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tag = get_wordnet_pos(word)\n",
    "    word_lemmatized = lemmatizer.lemmatize(word, pos_tag)\n",
    "\n",
    "    if pos_tag == \"r\" or pos_tag == \"R\":\n",
    "        try:\n",
    "            lemmas = wordnet.synset(word+'.r.1').lemmas()\n",
    "            pertainyms = lemmas[0].pertainyms()\n",
    "            name = pertainyms[0].name()\n",
    "            return name\n",
    "        except Exception:\n",
    "            return word_lemmatized\n",
    "    else:\n",
    "        return word_lemmatized\n",
    "\n",
    "def lemmatize_noun(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return word_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_accuracy(target, output):\n",
    "    output = output.argmax(axis=1) # -> multi label\n",
    "\n",
    "    tot_right = np.sum(target == output)\n",
    "    tot = target.size\n",
    "\n",
    "    return (tot_right/tot) * 100\n",
    "\n",
    "def get_binary_mean_accuracy(target, output):\n",
    "    eps = 1e-20\n",
    "    output = output.argmax(axis=1)\n",
    "\n",
    "    # TP + FN\n",
    "    gt_pos = np.sum((target == 1), axis=0).astype(float)\n",
    "    # TN + FP\n",
    "    gt_neg = np.sum((target == 0), axis=0).astype(float)\n",
    "    # TP\n",
    "    true_pos = np.sum((target == 1) * (output == 1), axis=0).astype(float)\n",
    "    # TN\n",
    "    true_neg = np.sum((target == 0) * (output == 0), axis=0).astype(float)\n",
    "\n",
    "    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive\n",
    "    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative\n",
    "    \n",
    "    # mean accuracy\n",
    "    return (label_pos_recall + label_neg_recall) / 2\n",
    "\n",
    "def get_evaluation_metrics(target, output, num_labels):\n",
    "    accuracy      = get_pred_accuracy(target, output, num_labels)\n",
    "    precision     = get_precision(target, output)\n",
    "    recall        = get_recall(target, output)\n",
    "    f1_score      = get_f1_score(target, output)\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "def infer(trainer, test_loader, num_labels):\n",
    "    predicts   = trainer.predict(test_loader)\n",
    "    soft       = torch.nn.Softmax(dim=1)\n",
    "    pred_probs = torch.from_numpy(predicts.predictions)\n",
    "    pred_probs = soft(pred_probs).numpy()\n",
    "    gt_list    = predicts.label_ids\n",
    "\n",
    "    return get_pred_accuracy(gt_list, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kombinasi variabel untuk setiap kategori\n",
    "categories = [\n",
    "    {\n",
    "        \"name\": \"attackVector\",\n",
    "        \"num_labels\": 4,\n",
    "        \"classes_names\": ['NETWORK', 'LOCAL', 'PHYSICAL', 'ADJACENT_NETWORK'],\n",
    "        \"label_position\": 1,\n",
    "        \"output_dir\": 'output1/attackVector'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"attackComplexity\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['LOW', 'HIGH'],\n",
    "        \"label_position\": 2,\n",
    "        \"output_dir\": 'output1/attackComplexity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"privilegeReq\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 3,\n",
    "        \"output_dir\": 'output1/privilegeReq'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"userInteraction\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['NONE', 'REQUIRED'],\n",
    "        \"label_position\": 4,\n",
    "        \"output_dir\": 'output1/userInteraction'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scope\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['UNCHANGED', 'CHANGED'],\n",
    "        \"label_position\": 5,\n",
    "        \"output_dir\": 'output1/scope'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"confidentiality\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 6,\n",
    "        \"output_dir\": 'output1/confidentiality'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"integrity\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 7,\n",
    "        \"output_dir\": 'output1/integrity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"availability\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 8,\n",
    "        \"output_dir\": 'output1/availability'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Device:  cuda\n",
      "GPU name is NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "\n",
      "### Training model for attackVector\n",
      "### Selecting Model and Tokenizer\n",
      "### Adding Tokens\n",
      "### Number of tokens in Tokenizer\n",
      "33867\n",
      "### Splitting Dataset\n",
      "### Lemmatizing Sentences\n",
      "### Tokenizing Sentences\n",
      "### Encoding Dataset\n",
      "### Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 500/18120 [02:39<1:32:05,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.52, 'grad_norm': 2.7700371742248535, 'learning_rate': 4.862030905077263e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1000/18120 [05:18<1:30:49,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4279, 'grad_norm': 2.299497127532959, 'learning_rate': 4.7240618101545256e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1500/18120 [07:55<1:28:26,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3959, 'grad_norm': 2.6241769790649414, 'learning_rate': 4.586092715231788e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2000/18120 [10:31<1:24:38,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3743, 'grad_norm': 7.912014484405518, 'learning_rate': 4.448123620309051e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2500/18120 [13:08<1:22:52,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3288, 'grad_norm': 0.3752608001232147, 'learning_rate': 4.310154525386314e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3000/18120 [15:44<1:19:20,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3559, 'grad_norm': 0.49020126461982727, 'learning_rate': 4.1721854304635764e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3500/18120 [18:21<1:17:32,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.342, 'grad_norm': 0.27111145853996277, 'learning_rate': 4.034216335540839e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4000/18120 [20:58<1:13:16,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.319, 'grad_norm': 0.8238117098808289, 'learning_rate': 3.896247240618102e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 4500/18120 [23:34<1:11:08,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.342, 'grad_norm': 4.729596138000488, 'learning_rate': 3.7582781456953645e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5000/18120 [26:11<1:08:13,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3278, 'grad_norm': 4.74317741394043, 'learning_rate': 3.620309050772627e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 5500/18120 [28:48<1:05:36,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2978, 'grad_norm': 6.090857028961182, 'learning_rate': 3.48233995584989e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6000/18120 [31:24<1:03:04,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3152, 'grad_norm': 6.858245372772217, 'learning_rate': 3.3443708609271526e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 6500/18120 [34:02<59:53,  3.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2872, 'grad_norm': 0.21792072057724, 'learning_rate': 3.206401766004415e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 7000/18120 [36:38<58:01,  3.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2575, 'grad_norm': 14.522615432739258, 'learning_rate': 3.068432671081678e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 7500/18120 [39:15<55:02,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2391, 'grad_norm': 0.05159483477473259, 'learning_rate': 2.9304635761589406e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8000/18120 [41:51<52:27,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.277, 'grad_norm': 4.436601161956787, 'learning_rate': 2.792494481236203e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8500/18120 [44:27<50:50,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2488, 'grad_norm': 6.297393321990967, 'learning_rate': 2.654525386313466e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 9000/18120 [47:04<47:41,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2567, 'grad_norm': 7.166050434112549, 'learning_rate': 2.5165562913907287e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 9500/18120 [49:40<45:41,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2541, 'grad_norm': 0.4552445411682129, 'learning_rate': 2.378587196467991e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 10000/18120 [52:16<42:13,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2368, 'grad_norm': 1.3384819030761719, 'learning_rate': 2.240618101545254e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 10500/18120 [54:53<39:48,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2342, 'grad_norm': 0.27500709891319275, 'learning_rate': 2.1026490066225165e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11000/18120 [57:30<37:01,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2516, 'grad_norm': 15.332067489624023, 'learning_rate': 1.9646799116997795e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 11500/18120 [1:00:06<35:06,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2448, 'grad_norm': 0.1511230617761612, 'learning_rate': 1.826710816777042e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 12000/18120 [1:02:42<31:44,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2256, 'grad_norm': 2.8520750999450684, 'learning_rate': 1.688741721854305e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 12500/18120 [1:05:20<29:21,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.234, 'grad_norm': 5.18375825881958, 'learning_rate': 1.5507726269315672e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13000/18120 [1:07:57<26:45,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1899, 'grad_norm': 76.68836212158203, 'learning_rate': 1.4128035320088301e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 13500/18120 [1:10:33<24:26,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1922, 'grad_norm': 7.9875688552856445, 'learning_rate': 1.274834437086093e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 14000/18120 [1:13:09<21:19,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1946, 'grad_norm': 5.972846031188965, 'learning_rate': 1.1368653421633555e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 14500/18120 [1:15:46<19:01,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1842, 'grad_norm': 7.217455863952637, 'learning_rate': 9.988962472406182e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15000/18120 [1:18:22<16:15,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1948, 'grad_norm': 0.5416224002838135, 'learning_rate': 8.609271523178809e-06, 'epoch': 2.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 15500/18120 [1:20:58<13:33,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1729, 'grad_norm': 0.7456077337265015, 'learning_rate': 7.229580573951435e-06, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16000/18120 [1:23:35<11:01,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1983, 'grad_norm': 34.45521926879883, 'learning_rate': 5.8498896247240626e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 16500/18120 [1:26:11<08:35,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1796, 'grad_norm': 0.24986600875854492, 'learning_rate': 4.470198675496689e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17000/18120 [1:28:47<05:49,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1848, 'grad_norm': 6.009553909301758, 'learning_rate': 3.090507726269316e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 17500/18120 [1:31:23<03:14,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1833, 'grad_norm': 9.270381927490234, 'learning_rate': 1.7108167770419427e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 18000/18120 [1:33:59<00:37,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'grad_norm': 0.039846546947956085, 'learning_rate': 3.3112582781456954e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18120/18120 [1:34:39<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5679.1806, 'train_samples_per_second': 25.525, 'train_steps_per_second': 3.191, 'train_loss': 0.2681960056422825, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3020/3020 [02:37<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for attackVector = 93.576159\n",
      "\n",
      "### Training model for attackComplexity\n",
      "### Selecting Model and Tokenizer\n",
      "### Adding Tokens\n",
      "### Number of tokens in Tokenizer\n",
      "33867\n",
      "### Splitting Dataset\n",
      "### Lemmatizing Sentences\n",
      "### Tokenizing Sentences\n",
      "### Encoding Dataset\n",
      "### Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 500/18120 [02:37<1:31:55,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1974, 'grad_norm': 0.083956778049469, 'learning_rate': 4.862030905077263e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1000/18120 [05:16<1:27:35,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.169, 'grad_norm': 0.25719785690307617, 'learning_rate': 4.7240618101545256e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1500/18120 [07:55<1:26:09,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1485, 'grad_norm': 0.060893166810274124, 'learning_rate': 4.586092715231788e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2000/18120 [10:33<1:26:04,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1641, 'grad_norm': 0.21197843551635742, 'learning_rate': 4.448123620309051e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2500/18120 [13:08<1:21:13,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.143, 'grad_norm': 0.22025230526924133, 'learning_rate': 4.310154525386314e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3000/18120 [15:45<1:18:49,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1476, 'grad_norm': 0.20624740421772003, 'learning_rate': 4.1721854304635764e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3500/18120 [18:21<1:16:31,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1428, 'grad_norm': 0.1841527372598648, 'learning_rate': 4.034216335540839e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4000/18120 [20:57<1:13:23,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1517, 'grad_norm': 0.1653788685798645, 'learning_rate': 3.896247240618102e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 4500/18120 [23:33<1:10:07,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1404, 'grad_norm': 0.2682121992111206, 'learning_rate': 3.7582781456953645e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5000/18120 [26:09<1:09:22,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1567, 'grad_norm': 0.2100558876991272, 'learning_rate': 3.620309050772627e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 5500/18120 [28:45<1:06:01,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1407, 'grad_norm': 0.15729781985282898, 'learning_rate': 3.48233995584989e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6000/18120 [31:21<1:03:13,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1389, 'grad_norm': 0.8957111239433289, 'learning_rate': 3.3443708609271526e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 6500/18120 [33:59<1:02:14,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1473, 'grad_norm': 0.14737693965435028, 'learning_rate': 3.206401766004415e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 7000/18120 [36:37<57:12,  3.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1377, 'grad_norm': 0.14239954948425293, 'learning_rate': 3.068432671081678e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7069/18120 [36:59<58:58,  3.12it/s]  "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global lemmatization\n",
    "\n",
    "    # variables\n",
    "    model_name = 'distilbert'\n",
    "    extra_tokens = True  # Menggunakan ekstra token\n",
    "    token_file = 'vocab/CVSS_5k.vocab'  # File token\n",
    "    lemmatization = True  # Menggunakan lemmatization\n",
    "\n",
    "    # Parameter untuk tuning\n",
    "    train_batch_size = 8  # Ukuran batch untuk training\n",
    "    test_batch_size = 4  # Ukuran batch untuk testing\n",
    "    epochs = 3  # Jumlah epoch\n",
    "    learning_rate = 5e-5  # Learning rate\n",
    "    weight_decay = 0  # Weight decay\n",
    "    warmup_steps = 0  # Jumlah warmup steps\n",
    "    warmup_ratio = 0  # Warmup ratio\n",
    "\n",
    "    # Periksa ketersediaan GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"### Device: \", device)\n",
    "    if torch.cuda.is_available():\n",
    "        devName = torch.cuda.get_device_name(0)\n",
    "        print(f\"GPU name is {devName}\")\n",
    "\n",
    " # Loop untuk setiap kategori\n",
    "    for category in categories:\n",
    "        print(f\"\\n### Training model for {category['name']}\")\n",
    "\n",
    "        # Directories and variables for the current category\n",
    "        output_dir = category[\"output_dir\"]\n",
    "        num_labels = category[\"num_labels\"]\n",
    "        classes_names = category[\"classes_names\"]\n",
    "        label_position = category[\"label_position\"]\n",
    "\n",
    "        # Buat output directory jika belum ada\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Select Model\n",
    "        tokenizer, model = select_tokenizer_model(model_name, extra_tokens=extra_tokens, token_file=token_file, num_labels=num_labels)\n",
    "\n",
    "        # Splitting Dataset\n",
    "        print(\"### Splitting Dataset\")\n",
    "\n",
    "        train_texts, train_labels = read_cvss_csv(f'data/train.csv', label_position, classes_names)\n",
    "        test_texts, test_labels = read_cvss_csv(f'data/test.csv', label_position, classes_names)\n",
    "\n",
    "        # Lemmatize Sentences\n",
    "        if lemmatization:\n",
    "            print(\"### Lemmatizing Sentences\")\n",
    "            lemmatized_train, lemmatized_test = lemmatize(train_texts, test_texts)\n",
    "\n",
    "        # Tokenize Sentences\n",
    "        print(\"### Tokenizing Sentences\")\n",
    "\n",
    "        if lemmatization:\n",
    "            train_encodings = tokenizer(lemmatized_train, truncation=True, padding=True)\n",
    "            test_encodings = tokenizer(lemmatized_test, truncation=True, padding=True)\n",
    "        else:\n",
    "            train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "            test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        # Dataset Encodings\n",
    "        print(\"### Encoding Dataset\")\n",
    "\n",
    "        train_dataset = CVSSDataset(train_encodings, train_labels)\n",
    "        test_dataset = CVSSDataset(test_encodings, test_labels)\n",
    "\n",
    "        # Training\n",
    "        print(\"### Training\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=test_batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            save_strategy=\"epoch\",\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_steps=warmup_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        acc = infer(trainer, test_dataset, num_labels)\n",
    "        print(f\"Accuracy for {category['name']} = {acc:.6f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from nltk import tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------- MODEL -------------------------------------\n",
    "\n",
    "def load_model(model_path):\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def select_tokenizer_model(model_name, extra_tokens, token_file, model_path, config_path):\n",
    "    global lemmatization\n",
    "    \n",
    "    if model_name == 'distilbert':\n",
    "        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
    "        config = DistilBertConfig.from_pretrained(config_path)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "        model = DistilBertForSequenceClassification(config)\n",
    "    \n",
    "    elif model_name == 'bert':\n",
    "        from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "        config = BertConfig.from_pretrained(config_path)\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'deberta':\n",
    "        from transformers import DebertaConfig, DebertaTokenizerFast, DebertaForSequenceClassification\n",
    "        config = DebertaConfig.from_pretrained(config_path)\n",
    "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
    "        model = DebertaForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'albert':\n",
    "        from transformers import AlbertConfig, AlbertTokenizerFast, AlbertForSequenceClassification\n",
    "        config = AlbertConfig.from_pretrained(config_path)\n",
    "        tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1')\n",
    "        model = AlbertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'roberta':\n",
    "        from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "        config = RobertaConfig.from_pretrained(config_path)\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "        \n",
    "    ### Add Tokens\n",
    "    if extra_tokens:\n",
    "        add_tokens_from_file(token_file, tokenizer, lemmatization)\n",
    "    number_tokens = len(tokenizer)\n",
    "\n",
    "    print(\"### Number of tokens in Tokenizer\")\n",
    "    print(number_tokens)\n",
    "\n",
    "    model.resize_token_embeddings(number_tokens) \n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def add_tokens_from_file(token_file, tokenizer, lemmatize=False):\n",
    "    print(\"### Adding Tokens\")\n",
    "    \n",
    "    file_      = open(token_file, 'r')\n",
    "    token_list = []\n",
    "    \n",
    "    for line in file_:\n",
    "        if lemmatize:\n",
    "            token_list.append(lemmatize_noun(line.rstrip(\"\\n\")))\n",
    "        else:\n",
    "            token_list.append(line.rstrip(\"\\n\"))\n",
    "    file_.close()\n",
    "    tokenizer.add_tokens(token_list)\n",
    "\n",
    "# -------------------------------------- METRICS -----------------------------------\n",
    "\n",
    "def get_pred_accuracy(target, output):\n",
    "    output = output.argmax(axis=1) # -> multi label\n",
    "\n",
    "    tot_right = np.sum(target == output)\n",
    "    tot = target.size\n",
    "\n",
    "    return (tot_right/tot) * 100\n",
    "\n",
    "def get_accuracy_score(target, output):\n",
    "    return accuracy_score(target, output)\n",
    "\n",
    "def get_f1_score(target, output):\n",
    "    return f1_score(target, output, average='weighted')\n",
    "\n",
    "def get_precision_score(target, output):\n",
    "    return precision_score(target, output, average='weighted')\n",
    "\n",
    "def get_recall_score(target, output):\n",
    "    return recall_score(target, output, average='weighted')\n",
    "\n",
    "def get_mean_accuracy(target, output):\n",
    "    eps = 1e-20\n",
    "    output = output.argmax(axis=1)\n",
    "\n",
    "    # TP + FN\n",
    "    gt_pos = np.sum((target == 1), axis=0).astype(float)\n",
    "    # TN + FP\n",
    "    gt_neg = np.sum((target == 0), axis=0).astype(float)\n",
    "    # TP\n",
    "    true_pos = np.sum((target == 1) * (output == 1), axis=0).astype(float)\n",
    "    # TN\n",
    "    true_neg = np.sum((target == 0) * (output == 0), axis=0).astype(float)\n",
    "\n",
    "    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive\n",
    "    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative\n",
    "    \n",
    "    # mean accuracy\n",
    "    return (label_pos_recall + label_neg_recall) / 2\n",
    "\n",
    "def get_balanced_accuracy(target, output):\n",
    "    return balanced_accuracy_score(target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kombinasi variabel untuk setiap kategori\n",
    "categories = [\n",
    "    {\n",
    "        \"name\": \"attackVector\",\n",
    "        \"num_labels\": 4,\n",
    "        \"classes_names\": ['NETWORK', 'LOCAL', 'PHYSICAL', 'ADJACENT_NETWORK'],\n",
    "        \"label_position\": 1,\n",
    "        \"output_dir\": 'output/attackVector'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"attackComplexity\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['LOW', 'HIGH'],\n",
    "        \"label_position\": 2,\n",
    "        \"output_dir\": 'output/attackComplexity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"privilegeReq\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 3,\n",
    "        \"output_dir\": 'output/privilegeReq'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"userInteraction\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['NONE', 'REQUIRED'],\n",
    "        \"label_position\": 4,\n",
    "        \"output_dir\": 'output/userInteraction'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scope\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['UNCHANGED', 'CHANGED'],\n",
    "        \"label_position\": 5,\n",
    "        \"output_dir\": 'output/scope'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"confidentiality\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 6,\n",
    "        \"output_dir\": 'output/confidentiality'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"integrity\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 7,\n",
    "        \"output_dir\": 'output/integrity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"availability\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 8,\n",
    "        \"output_dir\": 'output/availability'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- MAIN -----------------------------------\n",
    "\n",
    "def main():\n",
    "    global lemmatization\n",
    "    # variables\n",
    "    model_name = 'distilbert'\n",
    "    extra_tokens = True  # Menggunakan ekstra token\n",
    "    token_file = 'vocab/CVSS_5k.vocab'  # File token\n",
    "    lemmatization = True  # Menggunakan lemmatization\n",
    "\n",
    "    batch_size     = 2\n",
    "    \n",
    "\n",
    "    # Periksa ketersediaan GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"### Device: \", device)\n",
    "    if torch.cuda.is_available():\n",
    "        devName = torch.cuda.get_device_name(0)\n",
    "        print(f\"GPU name is {devName}\")\n",
    "\n",
    "    # Loop untuk setiap kategori\n",
    "    for category in categories:\n",
    "        print(f\"\\n### Training model for {category['name']}\")\n",
    "        # Directories and variables for the current category\n",
    "        root_dir = category[\"output_dir\"]\n",
    "        num_labels = category[\"num_labels\"]\n",
    "        list_classes = category[\"classes_names\"]\n",
    "        label_position = category[\"label_position\"]\n",
    "        model_path  = root_dir \n",
    "        config_path = root_dir + '/' + 'config.json'\n",
    "\n",
    "        ### Select Model\n",
    "        tokenizer, model = select_tokenizer_model(model_name, extra_tokens, token_file, model_path, config_path)\n",
    "\n",
    "        ### Load Dataset\n",
    "        print(\"### Loading Dataset\")\n",
    "        \n",
    "        test_texts, test_labels = read_cvss_csv('data/train.csv', label_position, list_classes)\n",
    "\n",
    "        ### Lemmatize Sentences\n",
    "        if lemmatization:\n",
    "            print(\"### Lemmatizing Sentences\")\n",
    "            lemmatized_test, _ = lemmatize(test_texts)\n",
    "\n",
    "        ### Tokenize Sentences\n",
    "        print(\"### Tokenizing Sentences\")\n",
    "\n",
    "        if lemmatization:\n",
    "            test_encodings = tokenizer(lemmatized_test, truncation=True, padding=True)\n",
    "            \n",
    "        else:\n",
    "            test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        ### Dataset Encodings\n",
    "        test_dataset = CVSSDataset(test_encodings, test_labels)\n",
    "\n",
    "        print(\"### Dataset Encodings\")\n",
    "\n",
    "        model = load_model(model_path)\n",
    "        model.to(device)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.eval()\n",
    "        pred_probs = []\n",
    "        gt_list = []\n",
    "\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            soft = torch.nn.Softmax(dim=1)\n",
    "            output_logits = soft(outputs.logits)\n",
    "            \n",
    "            gt_list.append(labels.cpu().detach().numpy())\n",
    "            pred_probs.append(output_logits.cpu().detach().numpy())\n",
    "\n",
    "        gt_list = np.concatenate(gt_list, axis=0)\n",
    "        pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "        pred_probs = pred_probs.argmax(axis=1)\n",
    "\n",
    "\n",
    "        print(\"Accuracy = {:.6f}   F1-score = {:.6f}   Precision = {:.6f}   Recall = {:.6f}   mean Accuracy = {:.6f}\".format(get_accuracy_score(gt_list, pred_probs), get_f1_score(gt_list, pred_probs), get_precision_score(gt_list, pred_probs), get_recall_score(gt_list, pred_probs), balanced_accuracy_score(gt_list, pred_probs)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semua model berhasil dimuat:\n",
      "['attack_vector', 'attack_complexity', 'privileges_required', 'user_interaction', 'scope', 'integrity_impact', 'confidentiality_impact', 'availability_impact']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Folder model dan nama kategori\n",
    "model_folders = {\n",
    "    \"attack_vector\": \"output/attackVector\",\n",
    "    \"attack_complexity\": \"output/attackComplexity\",\n",
    "    \"privileges_required\": \"output/privilegeReq\",\n",
    "    \"user_interaction\": \"output/userinteraction\",\n",
    "    \"scope\": \"output/scope\",\n",
    "    \"integrity_impact\": \"output/integrity\",\n",
    "    \"confidentiality_impact\": \"output/confidentiality\",\n",
    "    \"availability_impact\": \"output/availability\",\n",
    "}\n",
    "\n",
    "# Memuat tokenizer (gunakan satu tokenizer karena semua model berbasis DistilBERT)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Memuat model\n",
    "models = {}\n",
    "for category, folder in model_folders.items():\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(folder)\n",
    "    models[category] = model\n",
    "\n",
    "# Menampilkan hasil pemuatan\n",
    "print(\"Semua model berhasil dimuat:\")\n",
    "print(list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil prediksi:\n",
      "attack_vector: 2\n",
      "attack_complexity: 0\n",
      "privileges_required: 0\n",
      "user_interaction: 0\n",
      "scope: 0\n",
      "integrity_impact: 2\n",
      "confidentiality_impact: 0\n",
      "availability_impact: 0\n"
     ]
    }
   ],
   "source": [
    "# Input teks\n",
    "input_text = \"Improper conditions check in some Intel(R) Ethernet Controllers 800 series Linux drivers before version 1.4.11 may allow an authenticated user to potentially enable information disclosure or denial of service via local access.\"\n",
    "\n",
    "# Tokenisasi input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Prediksi dengan semua model\n",
    "outputs = {}\n",
    "for category, model in models.items():\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "        outputs[category] = prediction\n",
    "\n",
    "# Menampilkan hasil prediksi\n",
    "print(\"Hasil prediksi:\")\n",
    "for category, prediction in outputs.items():\n",
    "    print(f\"{category}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Predicted class: 0\n",
      "Predicted probabilities: tensor([[0.9554, 0.0376, 0.0070]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# -------------------------------------- MODEL -------------------------------------\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the pre-trained model from the specified path.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_name, extra_tokens, token_file, model_path, config_path):\n",
    "    \"\"\"\n",
    "    Load the tokenizer and model based on the model name and additional configurations.\n",
    "    \"\"\"\n",
    "    if model_name == 'distilbert':\n",
    "        from transformers import DistilBertTokenizerFast\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "    elif model_name == 'bert':\n",
    "        from transformers import BertTokenizerFast\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    elif model_name == 'deberta':\n",
    "        from transformers import DebertaTokenizerFast\n",
    "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
    "    elif model_name == 'roberta':\n",
    "        from transformers import RobertaTokenizerFast\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type {model_name}\")\n",
    "\n",
    "    # Add custom tokens if provided\n",
    "    if extra_tokens:\n",
    "        add_tokens_from_file(token_file, tokenizer)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def add_tokens_from_file(token_file, tokenizer):\n",
    "    \"\"\"\n",
    "    Add tokens from a file to the tokenizer.\n",
    "    \"\"\"\n",
    "    with open(token_file, 'r') as file:\n",
    "        token_list = [line.strip() for line in file]\n",
    "    tokenizer.add_tokens(token_list)\n",
    "\n",
    "# -------------------------------------- PREDICTION --------------------------------\n",
    "\n",
    "def predict(input_description, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Make a prediction for a single input description using the trained model and tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_description, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        probs = softmax(logits)\n",
    "    \n",
    "    predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    return predicted_class, probs\n",
    "\n",
    "# -------------------------------------- MAIN -----------------------------------\n",
    "\n",
    "def main():\n",
    "    # Set the device (GPU or CPU)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Paths and configurations\n",
    "    model_path = \"output/privilegeReq//\"\n",
    "    config_path = \"output/config.json\"\n",
    "    tokenizer_model_name = 'distilbert'  # Change to your model type (e.g., bert, roberta, etc.)\n",
    "    extra_tokens = False  # Set to True if you want to load extra tokens\n",
    "    token_file = \"vocab/CVSS_5k.vocab\"\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = load_model(model_path).to(device)\n",
    "    tokenizer = load_tokenizer(tokenizer_model_name, extra_tokens, token_file, model_path, config_path)\n",
    "\n",
    "    # Example input description\n",
    "    input_description = \"A vulnerability in Cisco Intercloud Fabric for Business and Cisco Intercloud Fabric for Providers could allow an unauthenticated, remote attacker to connect to the database used by these products. More Information: CSCus99394. Known Affected Releases: 7.3(0)ZN(0.99).\"\n",
    "\n",
    "    # Get the predicted class and probabilities\n",
    "    predicted_class, probs = predict(input_description, model, tokenizer, device)\n",
    "    \n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Predicted probabilities: {probs}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK,LOW,LOW,NONE,UNCHANGED,HIGH,HIGH,HIGH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
