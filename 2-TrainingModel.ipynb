{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import csv\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def read_cvss_txt(split_dir, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a directory structure and returns texts and labels.\n",
    "    Assumes directories named with class labels (e.g., LOW, HIGH).\n",
    "    \"\"\"\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"LOW\", \"HIGH\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == label_dir:\n",
    "                    labels.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def read_cvss_csv(file_name, num_label, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing texts and labels, and returns the texts and corresponding integer labels.\n",
    "    This function handles UTF-8 encoding to avoid issues with non-ASCII characters.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # Use 'with open' to ensure the file is properly closed after reading\n",
    "    with open(file_name, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "        \n",
    "        # Skip header row if it exists\n",
    "        next(csv_reader, None)  # This will skip the header, if present\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            texts.append(row[0])  # Assuming the first column is the text\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == row[num_label]:  # Match the label with classes\n",
    "                    labels.append(i)\n",
    "                    break  # Exit the loop once a match is found\n",
    "\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tokenizer_model(model_name, extra_tokens, token_file, num_labels):\n",
    "    global lemmatization\n",
    "\n",
    "    print(\"### Selecting Model and Tokenizer\")\n",
    "\n",
    "    if model_name == 'distilbert':\n",
    "        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
    "        config = DistilBertConfig.from_pretrained('distilbert-base-cased')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "        model = DistilBertForSequenceClassification(config)\n",
    "    \n",
    "    elif model_name == 'bert':\n",
    "        from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'deberta':\n",
    "        from transformers import DebertaConfig, DebertaTokenizerFast, DebertaForSequenceClassification\n",
    "        config = DebertaConfig.from_pretrained('microsoft/deberta-base')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
    "        model = DebertaForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'albert':\n",
    "        from transformers import AlbertConfig, AlbertTokenizerFast, AlbertForSequenceClassification\n",
    "        config = AlbertConfig.from_pretrained('albert-base-v1')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1')\n",
    "        model = AlbertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'roberta':\n",
    "        from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "        config = RobertaConfig.from_pretrained('roberta-base')\n",
    "        config.num_labels = num_labels\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "\n",
    "    ### Add Tokens\n",
    "    if extra_tokens:\n",
    "        add_tokens_from_file(token_file, tokenizer, lemmatization)\n",
    "    number_tokens = len(tokenizer)\n",
    "\n",
    "    print(\"### Number of tokens in Tokenizer\")\n",
    "    print(number_tokens)\n",
    "\n",
    "    # print(\"### Configuration\")\n",
    "    # print(model.config)\n",
    "\n",
    "    model.resize_token_embeddings(number_tokens) \n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def add_tokens_from_file(token_file, tokenizer, lemmatize=False):\n",
    "    print(\"### Adding Tokens\")\n",
    "    \n",
    "    file_      = open(token_file, 'r')\n",
    "    token_list = []\n",
    "    \n",
    "    for line in file_:\n",
    "        if lemmatize:\n",
    "            token_list.append(lemmatize_noun(line.rstrip(\"\\n\")))\n",
    "        else:\n",
    "            token_list.append(line.rstrip(\"\\n\"))\n",
    "    file_.close()\n",
    "    tokenizer.add_tokens(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    word_list = word_tokenize(sentence)\n",
    "    # lemmatized_output = ' '.join([lemmatize_word(w) for w in word_list]) # ALL LEMMATIZATION\n",
    "    lemmatized_output = ' '.join([lemmatize_noun(w) for w in word_list]) # NOUN LEMMATIZATION (OLD)\n",
    "\n",
    "    return lemmatized_output\n",
    "\n",
    "def lemmatize(train_texts, test_texts=None):\n",
    "    ### Lemmatize Sentences\n",
    "    lemmatized_texts_train = []\n",
    "    lemmatized_texts_test  = []\n",
    "    for text in train_texts:\n",
    "        lemmatized_texts_train.append(lemmatize_sentence(text))\n",
    "    if test_texts is not None:\n",
    "        for text in test_texts:\n",
    "            lemmatized_texts_test.append(lemmatize_sentence(text))\n",
    "\n",
    "    return lemmatized_texts_train, lemmatized_texts_test\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tag = get_wordnet_pos(word)\n",
    "    word_lemmatized = lemmatizer.lemmatize(word, pos_tag)\n",
    "\n",
    "    if pos_tag == \"r\" or pos_tag == \"R\":\n",
    "        try:\n",
    "            lemmas = wordnet.synset(word+'.r.1').lemmas()\n",
    "            pertainyms = lemmas[0].pertainyms()\n",
    "            name = pertainyms[0].name()\n",
    "            return name\n",
    "        except Exception:\n",
    "            return word_lemmatized\n",
    "    else:\n",
    "        return word_lemmatized\n",
    "\n",
    "def lemmatize_noun(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return word_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_accuracy(target, output):\n",
    "    output = output.argmax(axis=1) # -> multi label\n",
    "\n",
    "    tot_right = np.sum(target == output)\n",
    "    tot = target.size\n",
    "\n",
    "    return (tot_right/tot) * 100\n",
    "\n",
    "def get_binary_mean_accuracy(target, output):\n",
    "    eps = 1e-20\n",
    "    output = output.argmax(axis=1)\n",
    "\n",
    "    # TP + FN\n",
    "    gt_pos = np.sum((target == 1), axis=0).astype(float)\n",
    "    # TN + FP\n",
    "    gt_neg = np.sum((target == 0), axis=0).astype(float)\n",
    "    # TP\n",
    "    true_pos = np.sum((target == 1) * (output == 1), axis=0).astype(float)\n",
    "    # TN\n",
    "    true_neg = np.sum((target == 0) * (output == 0), axis=0).astype(float)\n",
    "\n",
    "    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive\n",
    "    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative\n",
    "    \n",
    "    # mean accuracy\n",
    "    return (label_pos_recall + label_neg_recall) / 2\n",
    "\n",
    "def get_evaluation_metrics(target, output, num_labels):\n",
    "    accuracy      = get_pred_accuracy(target, output, num_labels)\n",
    "    precision     = get_precision(target, output)\n",
    "    recall        = get_recall(target, output)\n",
    "    f1_score      = get_f1_score(target, output)\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "def infer(trainer, test_loader, num_labels):\n",
    "    predicts   = trainer.predict(test_loader)\n",
    "    soft       = torch.nn.Softmax(dim=1)\n",
    "    pred_probs = torch.from_numpy(predicts.predictions)\n",
    "    pred_probs = soft(pred_probs).numpy()\n",
    "    gt_list    = predicts.label_ids\n",
    "\n",
    "    return get_pred_accuracy(gt_list, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kombinasi variabel untuk setiap kategori\n",
    "categories = [\n",
    "    {\n",
    "        \"name\": \"attackVector\",\n",
    "        \"num_labels\": 4,\n",
    "        \"classes_names\": ['NETWORK', 'LOCAL', 'PHYSICAL', 'ADJACENT_NETWORK'],\n",
    "        \"label_position\": 1,\n",
    "        \"output_dir\": 'output/attackVector'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"attackComplexity\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['LOW', 'HIGH'],\n",
    "        \"label_position\": 2,\n",
    "        \"output_dir\": 'output/attackComplexity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"privilegeReq\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 3,\n",
    "        \"output_dir\": 'output/privilegeReq'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"userInteraction\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['NONE', 'REQUIRED'],\n",
    "        \"label_position\": 4,\n",
    "        \"output_dir\": 'output/userInteraction'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scope\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['UNCHANGED', 'CHANGED'],\n",
    "        \"label_position\": 5,\n",
    "        \"output_dir\": 'output/scope'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"confidentiality\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 6,\n",
    "        \"output_dir\": 'output/confidentiality'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"integrity\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 7,\n",
    "        \"output_dir\": 'output/integrity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"availability\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 8,\n",
    "        \"output_dir\": 'output/availability'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global lemmatization\n",
    "\n",
    "    # variables\n",
    "    model_name = 'distilbert'\n",
    "    extra_tokens = True  # Menggunakan ekstra token\n",
    "    token_file = 'vocab/CVSS_5k.vocab'  # File token\n",
    "    lemmatization = True  # Menggunakan lemmatization\n",
    "\n",
    "    # Parameter untuk tuning\n",
    "    train_batch_size = 8  # Ukuran batch untuk training\n",
    "    test_batch_size = 4  # Ukuran batch untuk testing\n",
    "    epochs = 3  # Jumlah epoch\n",
    "    learning_rate = 5e-5  # Learning rate\n",
    "    weight_decay = 0  # Weight decay\n",
    "    warmup_steps = 0  # Jumlah warmup steps\n",
    "    warmup_ratio = 0  # Warmup ratio\n",
    "\n",
    "    # Periksa ketersediaan GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"### Device: \", device)\n",
    "    if torch.cuda.is_available():\n",
    "        devName = torch.cuda.get_device_name(0)\n",
    "        print(f\"GPU name is {devName}\")\n",
    "\n",
    " # Loop untuk setiap kategori\n",
    "    for category in categories:\n",
    "        print(f\"\\n### Training model for {category['name']}\")\n",
    "\n",
    "        # Directories and variables for the current category\n",
    "        output_dir = category[\"output_dir\"]\n",
    "        num_labels = category[\"num_labels\"]\n",
    "        classes_names = category[\"classes_names\"]\n",
    "        label_position = category[\"label_position\"]\n",
    "\n",
    "        # Buat output directory jika belum ada\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Select Model\n",
    "        tokenizer, model = select_tokenizer_model(model_name, extra_tokens=extra_tokens, token_file=token_file, num_labels=num_labels)\n",
    "\n",
    "        # Splitting Dataset\n",
    "        print(\"### Splitting Dataset\")\n",
    "\n",
    "        train_texts, train_labels = read_cvss_csv(f'data/train.csv', label_position, classes_names)\n",
    "        test_texts, test_labels = read_cvss_csv(f'data/test.csv', label_position, classes_names)\n",
    "\n",
    "        # Lemmatize Sentences\n",
    "        if lemmatization:\n",
    "            print(\"### Lemmatizing Sentences\")\n",
    "            lemmatized_train, lemmatized_test = lemmatize(train_texts, test_texts)\n",
    "\n",
    "        # Tokenize Sentences\n",
    "        print(\"### Tokenizing Sentences\")\n",
    "\n",
    "        if lemmatization:\n",
    "            train_encodings = tokenizer(lemmatized_train, truncation=True, padding=True)\n",
    "            test_encodings = tokenizer(lemmatized_test, truncation=True, padding=True)\n",
    "        else:\n",
    "            train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "            test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        # Dataset Encodings\n",
    "        print(\"### Encoding Dataset\")\n",
    "\n",
    "        train_dataset = CVSSDataset(train_encodings, train_labels)\n",
    "        test_dataset = CVSSDataset(test_encodings, test_labels)\n",
    "\n",
    "        # Training\n",
    "        print(\"### Training\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=test_batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            save_strategy=\"epoch\",\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_steps=warmup_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        acc = infer(trainer, test_dataset, num_labels)\n",
    "        print(f\"Accuracy for {category['name']} = {acc:.6f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
