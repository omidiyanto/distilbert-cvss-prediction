{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/omidiyanto/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/omidiyanto/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/omidiyanto/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/omidiyanto/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/omidiyanto/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from nltk import tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def read_cvss_txt(split_dir, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a directory structure and returns texts and labels.\n",
    "    Assumes directories named with class labels (e.g., LOW, HIGH).\n",
    "    \"\"\"\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"LOW\", \"HIGH\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == label_dir:\n",
    "                    labels.append(i)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def read_cvss_csv(file_name, num_label, list_classes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing texts and labels, and returns the texts and corresponding integer labels.\n",
    "    This function handles UTF-8 encoding to avoid issues with non-ASCII characters.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # Use 'with open' to ensure the file is properly closed after reading\n",
    "    with open(file_name, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "        \n",
    "        # Skip header row if it exists\n",
    "        next(csv_reader, None)  # This will skip the header, if present\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            texts.append(row[0])  # Assuming the first column is the text\n",
    "            for i in range(len(list_classes)):\n",
    "                if list_classes[i] == row[num_label]:  # Match the label with classes\n",
    "                    labels.append(i)\n",
    "                    break  # Exit the loop once a match is found\n",
    "\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    word_list = word_tokenize(sentence)\n",
    "    # lemmatized_output = ' '.join([lemmatize_word(w) for w in word_list]) # ALL LEMMATIZATION\n",
    "    lemmatized_output = ' '.join([lemmatize_noun(w) for w in word_list]) # NOUN LEMMATIZATION (OLD)\n",
    "\n",
    "    return lemmatized_output\n",
    "\n",
    "def lemmatize(train_texts, test_texts=None):\n",
    "    ### Lemmatize Sentences\n",
    "    lemmatized_texts_train = []\n",
    "    lemmatized_texts_test  = []\n",
    "    for text in train_texts:\n",
    "        lemmatized_texts_train.append(lemmatize_sentence(text))\n",
    "    if test_texts is not None:\n",
    "        for text in test_texts:\n",
    "            lemmatized_texts_test.append(lemmatize_sentence(text))\n",
    "\n",
    "    return lemmatized_texts_train, lemmatized_texts_test\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tag = get_wordnet_pos(word)\n",
    "    word_lemmatized = lemmatizer.lemmatize(word, pos_tag)\n",
    "\n",
    "    if pos_tag == \"r\" or pos_tag == \"R\":\n",
    "        try:\n",
    "            lemmas = wordnet.synset(word+'.r.1').lemmas()\n",
    "            pertainyms = lemmas[0].pertainyms()\n",
    "            name = pertainyms[0].name()\n",
    "            return name\n",
    "        except Exception:\n",
    "            return word_lemmatized\n",
    "    else:\n",
    "        return word_lemmatized\n",
    "\n",
    "def lemmatize_noun(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return word_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------- MODEL -------------------------------------\n",
    "\n",
    "def load_model(model_path):\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def select_tokenizer_model(model_name, extra_tokens, token_file, model_path, config_path):\n",
    "    global lemmatization\n",
    "    \n",
    "    if model_name == 'distilbert':\n",
    "        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
    "        config = DistilBertConfig.from_pretrained(config_path)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "        model = DistilBertForSequenceClassification(config)\n",
    "    \n",
    "    elif model_name == 'bert':\n",
    "        from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
    "        config = BertConfig.from_pretrained(config_path)\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'deberta':\n",
    "        from transformers import DebertaConfig, DebertaTokenizerFast, DebertaForSequenceClassification\n",
    "        config = DebertaConfig.from_pretrained(config_path)\n",
    "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
    "        model = DebertaForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'albert':\n",
    "        from transformers import AlbertConfig, AlbertTokenizerFast, AlbertForSequenceClassification\n",
    "        config = AlbertConfig.from_pretrained(config_path)\n",
    "        tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1')\n",
    "        model = AlbertForSequenceClassification(config)\n",
    "\n",
    "    elif model_name == 'roberta':\n",
    "        from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "        config = RobertaConfig.from_pretrained(config_path)\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification(config)\n",
    "        \n",
    "    ### Add Tokens\n",
    "    if extra_tokens:\n",
    "        add_tokens_from_file(token_file, tokenizer, lemmatization)\n",
    "    number_tokens = len(tokenizer)\n",
    "\n",
    "    print(\"### Number of tokens in Tokenizer\")\n",
    "    print(number_tokens)\n",
    "\n",
    "    model.resize_token_embeddings(number_tokens) \n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def add_tokens_from_file(token_file, tokenizer, lemmatize=False):\n",
    "    print(\"### Adding Tokens\")\n",
    "    \n",
    "    file_      = open(token_file, 'r')\n",
    "    token_list = []\n",
    "    \n",
    "    for line in file_:\n",
    "        if lemmatize:\n",
    "            token_list.append(lemmatize_noun(line.rstrip(\"\\n\")))\n",
    "        else:\n",
    "            token_list.append(line.rstrip(\"\\n\"))\n",
    "    file_.close()\n",
    "    tokenizer.add_tokens(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- METRICS -----------------------------------\n",
    "\n",
    "def get_pred_accuracy(target, output):\n",
    "    output = output.argmax(axis=1) # -> multi label\n",
    "\n",
    "    tot_right = np.sum(target == output)\n",
    "    tot = target.size\n",
    "\n",
    "    return (tot_right/tot) * 100\n",
    "\n",
    "def get_accuracy_score(target, output):\n",
    "    return accuracy_score(target, output)\n",
    "\n",
    "def get_f1_score(target, output):\n",
    "    return f1_score(target, output, average='weighted')\n",
    "\n",
    "def get_precision_score(target, output):\n",
    "    return precision_score(target, output, average='weighted')\n",
    "\n",
    "def get_recall_score(target, output):\n",
    "    return recall_score(target, output, average='weighted')\n",
    "\n",
    "def get_mean_accuracy(target, output):\n",
    "    eps = 1e-20\n",
    "    output = output.argmax(axis=1)\n",
    "\n",
    "    # TP + FN\n",
    "    gt_pos = np.sum((target == 1), axis=0).astype(float)\n",
    "    # TN + FP\n",
    "    gt_neg = np.sum((target == 0), axis=0).astype(float)\n",
    "    # TP\n",
    "    true_pos = np.sum((target == 1) * (output == 1), axis=0).astype(float)\n",
    "    # TN\n",
    "    true_neg = np.sum((target == 0) * (output == 0), axis=0).astype(float)\n",
    "\n",
    "    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive\n",
    "    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative\n",
    "    \n",
    "    # mean accuracy\n",
    "    return (label_pos_recall + label_neg_recall) / 2\n",
    "\n",
    "def get_balanced_accuracy(target, output):\n",
    "    return balanced_accuracy_score(target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kombinasi variabel untuk setiap kategori\n",
    "categories = [\n",
    "    {\n",
    "        \"name\": \"attackVector\",\n",
    "        \"num_labels\": 4,\n",
    "        \"classes_names\": ['NETWORK', 'LOCAL', 'PHYSICAL', 'ADJACENT_NETWORK'],\n",
    "        \"label_position\": 1,\n",
    "        \"output_dir\": 'output/attackVector'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"attackComplexity\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['LOW', 'HIGH'],\n",
    "        \"label_position\": 2,\n",
    "        \"output_dir\": 'output/attackComplexity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"privilegeReq\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 3,\n",
    "        \"output_dir\": 'output/privilegeReq'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"userInteraction\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['NONE', 'REQUIRED'],\n",
    "        \"label_position\": 4,\n",
    "        \"output_dir\": 'output/userInteraction'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scope\",\n",
    "        \"num_labels\": 2,\n",
    "        \"classes_names\": ['UNCHANGED', 'CHANGED'],\n",
    "        \"label_position\": 5,\n",
    "        \"output_dir\": 'output/scope'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"confidentiality\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 6,\n",
    "        \"output_dir\": 'output/confidentiality'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"integrity\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 7,\n",
    "        \"output_dir\": 'output/integrity'\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"availability\",\n",
    "        \"num_labels\": 3,\n",
    "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
    "        \"label_position\": 8,\n",
    "        \"output_dir\": 'output/availability'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- MAIN -----------------------------------\n",
    "\n",
    "def main():\n",
    "    global lemmatization\n",
    "    # variables\n",
    "    model_name = 'distilbert'\n",
    "    extra_tokens = True  # Menggunakan ekstra token\n",
    "    token_file = 'vocab/CVSS_5k.vocab'  # File token\n",
    "    lemmatization = True  # Menggunakan lemmatization\n",
    "\n",
    "    batch_size     = 2\n",
    "    \n",
    "\n",
    "    # Periksa ketersediaan GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"### Device: \", device)\n",
    "    if torch.cuda.is_available():\n",
    "        devName = torch.cuda.get_device_name(0)\n",
    "        print(f\"GPU name is {devName}\")\n",
    "\n",
    "    # Loop untuk setiap kategori\n",
    "    for category in categories:\n",
    "        print(f\"\\n### Training model for {category['name']}\")\n",
    "        # Directories and variables for the current category\n",
    "        root_dir = category[\"output_dir\"]\n",
    "        num_labels = category[\"num_labels\"]\n",
    "        list_classes = category[\"classes_names\"]\n",
    "        label_position = category[\"label_position\"]\n",
    "        model_path  = root_dir \n",
    "        config_path = root_dir + '/' + 'config.json'\n",
    "\n",
    "        ### Select Model\n",
    "        tokenizer, model = select_tokenizer_model(model_name, extra_tokens, token_file, model_path, config_path)\n",
    "\n",
    "        ### Load Dataset\n",
    "        print(\"### Loading Dataset\")\n",
    "        \n",
    "        test_texts, test_labels = read_cvss_csv('data/train.csv', label_position, list_classes)\n",
    "\n",
    "        ### Lemmatize Sentences\n",
    "        if lemmatization:\n",
    "            print(\"### Lemmatizing Sentences\")\n",
    "            lemmatized_test, _ = lemmatize(test_texts)\n",
    "\n",
    "        ### Tokenize Sentences\n",
    "        print(\"### Tokenizing Sentences\")\n",
    "\n",
    "        if lemmatization:\n",
    "            test_encodings = tokenizer(lemmatized_test, truncation=True, padding=True)\n",
    "            \n",
    "        else:\n",
    "            test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        ### Dataset Encodings\n",
    "        test_dataset = CVSSDataset(test_encodings, test_labels)\n",
    "\n",
    "        print(\"### Dataset Encodings\")\n",
    "\n",
    "        model = load_model(model_path)\n",
    "        model.to(device)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.eval()\n",
    "        pred_probs = []\n",
    "        gt_list = []\n",
    "\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            soft = torch.nn.Softmax(dim=1)\n",
    "            output_logits = soft(outputs.logits)\n",
    "            \n",
    "            gt_list.append(labels.cpu().detach().numpy())\n",
    "            pred_probs.append(output_logits.cpu().detach().numpy())\n",
    "\n",
    "        gt_list = np.concatenate(gt_list, axis=0)\n",
    "        pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "        pred_probs = pred_probs.argmax(axis=1)\n",
    "\n",
    "\n",
    "        print(\"Accuracy = {:.6f}   F1-score = {:.6f}   Precision = {:.6f}   Recall = {:.6f}   mean Accuracy = {:.6f}\".format(get_accuracy_score(gt_list, pred_probs), get_f1_score(gt_list, pred_probs), get_precision_score(gt_list, pred_probs), get_recall_score(gt_list, pred_probs), balanced_accuracy_score(gt_list, pred_probs)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
