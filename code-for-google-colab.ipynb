{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1><b>DOWNLOAD DATASET + EDA</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00IY4Xq6hMtP",
        "outputId": "d7745b1d-3a95-41b9-e94d-b242722e3c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data for year 2022...\n",
            "Processing data for year 2022: 25287 total CVEs\n",
            "Downloading data for year 2023...\n",
            "Processing data for year 2023: 29079 total CVEs\n",
            "Downloading data for year 2024...\n",
            "Processing data for year 2024: 32859 total CVEs\n",
            "Processed CVEs: 60400\n",
            "Skipped CVEs: 26825\n",
            "Data berhasil dipisahkan menjadi 'data/train.csv' dan 'data/test.csv'.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 60400 entries, 0 to 60399\n",
            "Data columns (total 9 columns):\n",
            " #   Column                         Non-Null Count  Dtype \n",
            "---  ------                         --------------  ----- \n",
            " 0   english_description            60400 non-null  object\n",
            " 1   cvssv3_attack_vector           60400 non-null  object\n",
            " 2   cvssv3_attack_complexity       60400 non-null  object\n",
            " 3   cvssv3_privileges_required     60400 non-null  object\n",
            " 4   cvssv3_user_interaction        60400 non-null  object\n",
            " 5   cvssv3_scope                   60400 non-null  object\n",
            " 6   cvssv3_confidentiality_impact  60400 non-null  object\n",
            " 7   cvssv3_integrity_impact        60400 non-null  object\n",
            " 8   cvssv3_availability_impact     60400 non-null  object\n",
            "dtypes: object(9)\n",
            "memory usage: 4.1+ MB\n",
            "None\n",
            "<bound method NDFrame.head of                                      english_description cvssv3_attack_vector  \\\n",
            "0      \"Non-transparent sharing of branch predictor s...                LOCAL   \n",
            "1      \"Non-transparent sharing of branch predictor w...                LOCAL   \n",
            "2      \"Hardware debug modes and processor INIT setti...             PHYSICAL   \n",
            "3      \"Sensitive information accessible by physical ...             PHYSICAL   \n",
            "4      \"Insertion of Sensitive Information into Log F...                LOCAL   \n",
            "...                                                  ...                  ...   \n",
            "60395  \"Enterprise Cloud Database from Ragic does not...              NETWORK   \n",
            "60396  \"A vulnerability was found in code-projects Bl...              NETWORK   \n",
            "60397  \"A post-authentication SQL Injection vulnerabi...              NETWORK   \n",
            "60398  \"A maliciously crafted DWG file when parsed in...                LOCAL   \n",
            "60399  \"A maliciously crafted DWG file when parsed in...                LOCAL   \n",
            "\n",
            "      cvssv3_attack_complexity cvssv3_privileges_required  \\\n",
            "0                          LOW                        LOW   \n",
            "1                          LOW                        LOW   \n",
            "2                          LOW                       NONE   \n",
            "3                          LOW                       NONE   \n",
            "4                          LOW                        LOW   \n",
            "...                        ...                        ...   \n",
            "60395                      LOW                       NONE   \n",
            "60396                      LOW                       NONE   \n",
            "60397                      LOW                        LOW   \n",
            "60398                      LOW                       NONE   \n",
            "60399                      LOW                       NONE   \n",
            "\n",
            "      cvssv3_user_interaction cvssv3_scope cvssv3_confidentiality_impact  \\\n",
            "0                        NONE      CHANGED                          HIGH   \n",
            "1                        NONE      CHANGED                          HIGH   \n",
            "2                        NONE    UNCHANGED                          HIGH   \n",
            "3                        NONE    UNCHANGED                           LOW   \n",
            "4                        NONE    UNCHANGED                          HIGH   \n",
            "...                       ...          ...                           ...   \n",
            "60395                    NONE    UNCHANGED                          HIGH   \n",
            "60396                    NONE    UNCHANGED                          HIGH   \n",
            "60397                    NONE    UNCHANGED                          HIGH   \n",
            "60398                REQUIRED    UNCHANGED                          HIGH   \n",
            "60399                REQUIRED    UNCHANGED                          HIGH   \n",
            "\n",
            "      cvssv3_integrity_impact cvssv3_availability_impact  \n",
            "0                        NONE                       NONE  \n",
            "1                        NONE                       NONE  \n",
            "2                        HIGH                       HIGH  \n",
            "3                        NONE                       NONE  \n",
            "4                        NONE                       NONE  \n",
            "...                       ...                        ...  \n",
            "60395                    HIGH                       HIGH  \n",
            "60396                    HIGH                       HIGH  \n",
            "60397                    HIGH                       HIGH  \n",
            "60398                    HIGH                       HIGH  \n",
            "60399                    HIGH                       HIGH  \n",
            "\n",
            "[60400 rows x 9 columns]>\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def download_nvd_feed(year):\n",
        "    feed_url = f'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-{year}.json.gz'\n",
        "    response = requests.get(feed_url)\n",
        "\n",
        "    with open(f'nvdcve-1.1-{year}.json.gz', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    return f'nvdcve-1.1-{year}.json.gz'\n",
        "\n",
        "\n",
        "def extract_cve_from_item(item):\n",
        "    if 'baseMetricV3' not in item['impact']:\n",
        "        return None\n",
        "\n",
        "    en_text = next(\n",
        "        (desc['value'] for desc in item['cve']['description']['description_data']\n",
        "         if desc['lang'] == 'en'), None)\n",
        "    if en_text is None:\n",
        "        return None\n",
        "\n",
        "    en_text = en_text.replace('\\n', ' ')\n",
        "\n",
        "    return {\n",
        "        'english_description': en_text,\n",
        "        'cvssv3_attack_vector': item['impact']['baseMetricV3']['cvssV3']['attackVector'],\n",
        "        'cvssv3_attack_complexity': item['impact']['baseMetricV3']['cvssV3']['attackComplexity'],\n",
        "        'cvssv3_privileges_required': item['impact']['baseMetricV3']['cvssV3']['privilegesRequired'],\n",
        "        'cvssv3_user_interaction': item['impact']['baseMetricV3']['cvssV3']['userInteraction'],\n",
        "        'cvssv3_scope': item['impact']['baseMetricV3']['cvssV3']['scope'],\n",
        "        'cvssv3_confidentiality_impact': item['impact']['baseMetricV3']['cvssV3']['confidentialityImpact'],\n",
        "        'cvssv3_integrity_impact': item['impact']['baseMetricV3']['cvssV3']['integrityImpact'],\n",
        "        'cvssv3_availability_impact': item['impact']['baseMetricV3']['cvssV3']['availabilityImpact'],\n",
        "    }\n",
        "\n",
        "def process_cve_data(years):\n",
        "    all_cves = []\n",
        "    skipped = 0\n",
        "    processed = 0\n",
        "\n",
        "    for year in years:\n",
        "        filename = f'nvdcve-1.1-{year}.json.gz'\n",
        "\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"Downloading data for year {year}...\")\n",
        "            filename = download_nvd_feed(year)\n",
        "        else:\n",
        "            print(f\"File for year {year} already exists, using the existing file.\")\n",
        "\n",
        "        with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "            nvd_data = json.load(f)\n",
        "\n",
        "        print(f\"Processing data for year {year}: {len(nvd_data['CVE_Items'])} total CVEs\")\n",
        "\n",
        "        year_cves = []\n",
        "        for item in nvd_data['CVE_Items']:\n",
        "            relevant_data = extract_cve_from_item(item)\n",
        "            if relevant_data is None:\n",
        "                skipped += 1\n",
        "            else:\n",
        "                year_cves.append(relevant_data)\n",
        "                processed += 1\n",
        "\n",
        "        all_cves.extend(year_cves)\n",
        "\n",
        "    print(f\"Processed CVEs: {processed}\\nSkipped CVEs: {skipped}\")\n",
        "    return pd.DataFrame(all_cves)\n",
        "\n",
        "\n",
        "years = [2022, 2023, 2024]\n",
        "cvss_data = process_cve_data(years)\n",
        "\n",
        "# Membuat kolom english_description diawali dengan tanda kutip\n",
        "cvss_data['english_description'] = cvss_data['english_description'].apply(lambda x: f'\"{x}\"')\n",
        "\n",
        "# CLEANING : Ganti semua newline dengan spasi di kolom 'english_description'\n",
        "cvss_data['english_description'] = cvss_data['english_description'].str.replace(r'[\\n\\r]+', ' ', regex=True)\n",
        "\n",
        "# Simpan data ke dalam file CSV jika diperlukan\n",
        "cvss_data.to_csv('cvss_data.csv', index=False)\n",
        "\n",
        "# Membuat direktori 'data' jika belum ada\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Memisahkan data menjadi training dan testing (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(cvss_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menyimpan data ke dalam file CSV\n",
        "train_data.to_csv('data/train.csv', index=False)\n",
        "test_data.to_csv('data/test.csv', index=False)\n",
        "\n",
        "print(\"Data berhasil dipisahkan menjadi 'data/train.csv' dan 'data/test.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_lUEJ-ikS5e"
      },
      "outputs": [],
      "source": [
        "!mkdir -p vocab\n",
        "!wget https://raw.githubusercontent.com/omidiyanto/distilbert-cvss-prediction/refs/heads/master/vocab/CVSS_5k.vocab -P vocab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cvss_data.describe(include='all'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cvss_data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cvss_data.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnique Values per Column:\")\n",
        "print(cvss_data.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values per kolom\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(cvss_data.isnull().sum())\n",
        "\n",
        "# Persentase missing values\n",
        "missing_percentage = cvss_data.isnull().mean() * 100\n",
        "print(\"\\nPercentage of Missing Values:\")\n",
        "print(missing_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualisasi distribusi per kategori\n",
        "categories = [\n",
        "    'cvssv3_attack_vector', 'cvssv3_attack_complexity', 'cvssv3_privileges_required',\n",
        "    'cvssv3_user_interaction', 'cvssv3_scope', 'cvssv3_confidentiality_impact',\n",
        "    'cvssv3_integrity_impact', 'cvssv3_availability_impact'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, category in enumerate(categories):\n",
        "    plt.subplot(4, 2, i + 1)\n",
        "    sns.countplot(y=category, data=cvss_data, order=cvss_data[category].value_counts().index)\n",
        "    plt.title(f\"Distribution of {category.replace('_', ' ').capitalize()}\")\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel(category.replace('_', ' ').capitalize())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cvss_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribusi setiap kategori\n",
        "for category in categories:\n",
        "    print(f\"\\nDistribusi untuk {category}:\")\n",
        "    print(cvss_data[category].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Menghitung panjang teks\n",
        "cvss_data['description_length'] = cvss_data['english_description'].apply(len)\n",
        "\n",
        "# Statistik deskriptif\n",
        "length_stats = cvss_data['description_length'].describe()\n",
        "print(length_stats)\n",
        "\n",
        "# Visualisasi distribusi panjang teks\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(cvss_data['description_length'], kde=True, bins=30, color='blue')\n",
        "plt.title('Distribusi Panjang Deskripsi')\n",
        "plt.xlabel('Panjang Deskripsi')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate word count and add it as a new column\n",
        "cvss_data['word_count'] = cvss_data['english_description'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Statistik deskriptif\n",
        "length_stats = cvss_data['word_count'].describe()\n",
        "print(length_stats)\n",
        "\n",
        "# Visualisasi distribusi panjang teks\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(cvss_data['word_count'], kde=True, bins=30, color='blue')\n",
        "plt.title('Distribusi Jumlah Kata Deskripsi')\n",
        "plt.xlabel('Jumlah Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mencari indeks teks terpanjang dan terpendek\n",
        "longest_index = cvss_data['english_description'].apply(len).idxmax()\n",
        "shortest_index = cvss_data['english_description'].apply(len).idxmin()\n",
        "\n",
        "# Mengambil teks terpanjang dan terpendek\n",
        "longest_text = cvss_data.loc[longest_index, 'english_description']\n",
        "shortest_text = cvss_data.loc[shortest_index, 'english_description']\n",
        "\n",
        "# Menampilkan teks terpanjang dan terpendek\n",
        "print(\"Teks Terpanjang:\\n\", longest_text)\n",
        "print(\"\\nTeks Terpendek:\\n\", shortest_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1><b>TRAINING MODEL</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR0C13Qg0dD",
        "outputId": "55f618e9-4543-4528-b75b-2747163b6af7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import csv\n",
        "from transformers import Trainer, TrainingArguments, AdamW\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wGqJah4Tg0dE"
      },
      "outputs": [],
      "source": [
        "class CVSSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def read_cvss_txt(split_dir, list_classes):\n",
        "    \"\"\"\n",
        "    Reads a directory structure and returns texts and labels.\n",
        "    Assumes directories named with class labels (e.g., LOW, HIGH).\n",
        "    \"\"\"\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"LOW\", \"HIGH\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            for i in range(len(list_classes)):\n",
        "                if list_classes[i] == label_dir:\n",
        "                    labels.append(i)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "def read_cvss_csv(file_name, num_label, list_classes):\n",
        "    \"\"\"\n",
        "    Reads a CSV file containing texts and labels, and returns the texts and corresponding integer labels.\n",
        "    This function handles UTF-8 encoding to avoid issues with non-ASCII characters.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # Use 'with open' to ensure the file is properly closed after reading\n",
        "    with open(file_name, 'r', encoding='utf-8') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
        "\n",
        "        # Skip header row if it exists\n",
        "        next(csv_reader, None)  # This will skip the header, if present\n",
        "\n",
        "        for row in csv_reader:\n",
        "            texts.append(row[0])  # Assuming the first column is the text\n",
        "            for i in range(len(list_classes)):\n",
        "                if list_classes[i] == row[num_label]:  # Match the label with classes\n",
        "                    labels.append(i)\n",
        "                    break  # Exit the loop once a match is found\n",
        "\n",
        "    return texts, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-27PrcZhg0dE"
      },
      "outputs": [],
      "source": [
        "def select_tokenizer_model(model_name, extra_tokens, token_file, num_labels):\n",
        "    global lemmatization\n",
        "\n",
        "    print(\"### Selecting Model and Tokenizer\")\n",
        "\n",
        "    if model_name == 'distilbert':\n",
        "        from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertConfig\n",
        "        config = DistilBertConfig.from_pretrained('distilbert-base-cased')\n",
        "        config.num_labels = num_labels\n",
        "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
        "        model = DistilBertForSequenceClassification(config)\n",
        "\n",
        "    elif model_name == 'bert':\n",
        "        from transformers import BertTokenizerFast, BertForSequenceClassification, BertConfig\n",
        "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "        config.num_labels = num_labels\n",
        "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification(config)\n",
        "\n",
        "    elif model_name == 'deberta':\n",
        "        from transformers import DebertaConfig, DebertaTokenizerFast, DebertaForSequenceClassification\n",
        "        config = DebertaConfig.from_pretrained('microsoft/deberta-base')\n",
        "        config.num_labels = num_labels\n",
        "        tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
        "        model = DebertaForSequenceClassification(config)\n",
        "\n",
        "    elif model_name == 'albert':\n",
        "        from transformers import AlbertConfig, AlbertTokenizerFast, AlbertForSequenceClassification\n",
        "        config = AlbertConfig.from_pretrained('albert-base-v1')\n",
        "        config.num_labels = num_labels\n",
        "        tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v1')\n",
        "        model = AlbertForSequenceClassification(config)\n",
        "\n",
        "    elif model_name == 'roberta':\n",
        "        from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "        config = RobertaConfig.from_pretrained('roberta-base')\n",
        "        config.num_labels = num_labels\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "        model = RobertaForSequenceClassification(config)\n",
        "\n",
        "    ### Add Tokens\n",
        "    if extra_tokens:\n",
        "        add_tokens_from_file(token_file, tokenizer, lemmatization)\n",
        "    number_tokens = len(tokenizer)\n",
        "\n",
        "    print(\"### Number of tokens in Tokenizer\")\n",
        "    print(number_tokens)\n",
        "\n",
        "    # print(\"### Configuration\")\n",
        "    # print(model.config)\n",
        "\n",
        "    model.resize_token_embeddings(number_tokens)\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def add_tokens_from_file(token_file, tokenizer, lemmatize=False):\n",
        "    print(\"### Adding Tokens\")\n",
        "\n",
        "    file_      = open(token_file, 'r')\n",
        "    token_list = []\n",
        "\n",
        "    for line in file_:\n",
        "        if lemmatize:\n",
        "            token_list.append(lemmatize_noun(line.rstrip(\"\\n\")))\n",
        "        else:\n",
        "            token_list.append(line.rstrip(\"\\n\"))\n",
        "    file_.close()\n",
        "    tokenizer.add_tokens(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O6THGJTUg0dE"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    word_list = word_tokenize(sentence)\n",
        "    # lemmatized_output = ' '.join([lemmatize_word(w) for w in word_list]) # ALL LEMMATIZATION\n",
        "    lemmatized_output = ' '.join([lemmatize_noun(w) for w in word_list]) # NOUN LEMMATIZATION (OLD)\n",
        "\n",
        "    return lemmatized_output\n",
        "\n",
        "def lemmatize(train_texts, test_texts=None):\n",
        "    ### Lemmatize Sentences\n",
        "    lemmatized_texts_train = []\n",
        "    lemmatized_texts_test  = []\n",
        "    for text in train_texts:\n",
        "        lemmatized_texts_train.append(lemmatize_sentence(text))\n",
        "    if test_texts is not None:\n",
        "        for text in test_texts:\n",
        "            lemmatized_texts_test.append(lemmatize_sentence(text))\n",
        "\n",
        "    return lemmatized_texts_train, lemmatized_texts_test\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tag = get_wordnet_pos(word)\n",
        "    word_lemmatized = lemmatizer.lemmatize(word, pos_tag)\n",
        "\n",
        "    if pos_tag == \"r\" or pos_tag == \"R\":\n",
        "        try:\n",
        "            lemmas = wordnet.synset(word+'.r.1').lemmas()\n",
        "            pertainyms = lemmas[0].pertainyms()\n",
        "            name = pertainyms[0].name()\n",
        "            return name\n",
        "        except Exception:\n",
        "            return word_lemmatized\n",
        "    else:\n",
        "        return word_lemmatized\n",
        "\n",
        "def lemmatize_noun(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_lemmatized = lemmatizer.lemmatize(word)\n",
        "\n",
        "    return word_lemmatized\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CeZelCntg0dF"
      },
      "outputs": [],
      "source": [
        "def get_pred_accuracy(target, output):\n",
        "    output = output.argmax(axis=1) # -> multi label\n",
        "\n",
        "    tot_right = np.sum(target == output)\n",
        "    tot = target.size\n",
        "\n",
        "    return (tot_right/tot) * 100\n",
        "\n",
        "def get_binary_mean_accuracy(target, output):\n",
        "    eps = 1e-20\n",
        "    output = output.argmax(axis=1)\n",
        "\n",
        "    # TP + FN\n",
        "    gt_pos = np.sum((target == 1), axis=0).astype(float)\n",
        "    # TN + FP\n",
        "    gt_neg = np.sum((target == 0), axis=0).astype(float)\n",
        "    # TP\n",
        "    true_pos = np.sum((target == 1) * (output == 1), axis=0).astype(float)\n",
        "    # TN\n",
        "    true_neg = np.sum((target == 0) * (output == 0), axis=0).astype(float)\n",
        "\n",
        "    label_pos_recall = 1.0 * true_pos / (gt_pos + eps)  # true positive\n",
        "    label_neg_recall = 1.0 * true_neg / (gt_neg + eps)  # true negative\n",
        "\n",
        "    # mean accuracy\n",
        "    return (label_pos_recall + label_neg_recall) / 2\n",
        "\n",
        "def get_evaluation_metrics(target, output, num_labels):\n",
        "    accuracy      = get_pred_accuracy(target, output, num_labels)\n",
        "    precision     = get_precision(target, output)\n",
        "    recall        = get_recall(target, output)\n",
        "    f1_score      = get_f1_score(target, output)\n",
        "\n",
        "    return accuracy, precision, recall, f1_score\n",
        "\n",
        "def infer(trainer, test_loader, num_labels):\n",
        "    predicts   = trainer.predict(test_loader)\n",
        "    soft       = torch.nn.Softmax(dim=1)\n",
        "    pred_probs = torch.from_numpy(predicts.predictions)\n",
        "    pred_probs = soft(pred_probs).numpy()\n",
        "    gt_list    = predicts.label_ids\n",
        "\n",
        "    return get_pred_accuracy(gt_list, pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ydE8ZSrgg0dF"
      },
      "outputs": [],
      "source": [
        "# Daftar kombinasi variabel untuk setiap kategori\n",
        "categories = [\n",
        "    {\n",
        "        \"name\": \"attackVector\",\n",
        "        \"num_labels\": 4,\n",
        "        \"classes_names\": ['NETWORK', 'LOCAL', 'PHYSICAL', 'ADJACENT_NETWORK'],\n",
        "        \"label_position\": 1,\n",
        "        \"output_dir\": 'output/attackVector'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"attackComplexity\",\n",
        "        \"num_labels\": 2,\n",
        "        \"classes_names\": ['LOW', 'HIGH'],\n",
        "        \"label_position\": 2,\n",
        "        \"output_dir\": 'output/attackComplexity'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"privilegeReq\",\n",
        "        \"num_labels\": 3,\n",
        "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
        "        \"label_position\": 3,\n",
        "        \"output_dir\": 'output/privilegeReq'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"userInteraction\",\n",
        "        \"num_labels\": 2,\n",
        "        \"classes_names\": ['NONE', 'REQUIRED'],\n",
        "        \"label_position\": 4,\n",
        "        \"output_dir\": 'output/userInteraction'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"scope\",\n",
        "        \"num_labels\": 2,\n",
        "        \"classes_names\": ['UNCHANGED', 'CHANGED'],\n",
        "        \"label_position\": 5,\n",
        "        \"output_dir\": 'output/scope'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"confidentiality\",\n",
        "        \"num_labels\": 3,\n",
        "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
        "        \"label_position\": 6,\n",
        "        \"output_dir\": 'output/confidentiality'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"integrity\",\n",
        "        \"num_labels\": 3,\n",
        "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
        "        \"label_position\": 7,\n",
        "        \"output_dir\": 'output/integrity'\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"availability\",\n",
        "        \"num_labels\": 3,\n",
        "        \"classes_names\": ['NONE', 'LOW', 'HIGH'],\n",
        "        \"label_position\": 8,\n",
        "        \"output_dir\": 'output/availability'\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "RJ67Igtsg0dF",
        "outputId": "ab197e7f-7be5-443c-fa44-2e5f42b3559e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Device:  cuda\n",
            "GPU name is Tesla T4\n",
            "\n",
            "### Training model for attackVector\n",
            "### Selecting Model and Tokenizer\n",
            "### Adding Tokens\n",
            "### Number of tokens in Tokenizer\n",
            "33867\n",
            "### Splitting Dataset\n",
            "### Lemmatizing Sentences\n",
            "### Tokenizing Sentences\n",
            "### Encoding Dataset\n",
            "### Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241228_183810-6rrwhnhj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/omidiyanto7-midiyanto/huggingface/runs/6rrwhnhj' target=\"_blank\">output/attackVector</a></strong> to <a href='https://wandb.ai/omidiyanto7-midiyanto/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/omidiyanto7-midiyanto/huggingface' target=\"_blank\">https://wandb.ai/omidiyanto7-midiyanto/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/omidiyanto7-midiyanto/huggingface/runs/6rrwhnhj' target=\"_blank\">https://wandb.ai/omidiyanto7-midiyanto/huggingface/runs/6rrwhnhj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='484' max='18120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  484/18120 02:45 < 1:41:02, 2.91 it/s, Epoch 0.08/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def main():\n",
        "    global lemmatization\n",
        "\n",
        "    # variables\n",
        "    model_name = 'distilbert'\n",
        "    extra_tokens = True  # Menggunakan ekstra token\n",
        "    token_file = 'vocab/CVSS_5k.vocab'  # File token\n",
        "    lemmatization = True  # Menggunakan lemmatization\n",
        "\n",
        "    # Parameter untuk tuning\n",
        "    train_batch_size = 8  # Ukuran batch untuk training\n",
        "    test_batch_size = 4  # Ukuran batch untuk testing\n",
        "    epochs = 3  # Jumlah epoch\n",
        "    learning_rate = 5e-5  # Learning rate\n",
        "    weight_decay = 0  # Weight decay\n",
        "    warmup_steps = 0  # Jumlah warmup steps\n",
        "    warmup_ratio = 0  # Warmup ratio\n",
        "\n",
        "    # Periksa ketersediaan GPU\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(\"### Device: \", device)\n",
        "    if torch.cuda.is_available():\n",
        "        devName = torch.cuda.get_device_name(0)\n",
        "        print(f\"GPU name is {devName}\")\n",
        "\n",
        " # Loop untuk setiap kategori\n",
        "    for category in categories:\n",
        "        print(f\"\\n### Training model for {category['name']}\")\n",
        "\n",
        "        # Directories and variables for the current category\n",
        "        output_dir = category[\"output_dir\"]\n",
        "        num_labels = category[\"num_labels\"]\n",
        "        classes_names = category[\"classes_names\"]\n",
        "        label_position = category[\"label_position\"]\n",
        "\n",
        "        # Buat output directory jika belum ada\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Select Model\n",
        "        tokenizer, model = select_tokenizer_model(model_name, extra_tokens=extra_tokens, token_file=token_file, num_labels=num_labels)\n",
        "\n",
        "        # Splitting Dataset\n",
        "        print(\"### Splitting Dataset\")\n",
        "\n",
        "        train_texts, train_labels = read_cvss_csv(f'data/train.csv', label_position, classes_names)\n",
        "        test_texts, test_labels = read_cvss_csv(f'data/test.csv', label_position, classes_names)\n",
        "\n",
        "        # Lemmatize Sentences\n",
        "        if lemmatization:\n",
        "            print(\"### Lemmatizing Sentences\")\n",
        "            lemmatized_train, lemmatized_test = lemmatize(train_texts, test_texts)\n",
        "\n",
        "        # Tokenize Sentences\n",
        "        print(\"### Tokenizing Sentences\")\n",
        "\n",
        "        if lemmatization:\n",
        "            train_encodings = tokenizer(lemmatized_train, truncation=True, padding=True)\n",
        "            test_encodings = tokenizer(lemmatized_test, truncation=True, padding=True)\n",
        "        else:\n",
        "            train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "            test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "        # Dataset Encodings\n",
        "        print(\"### Encoding Dataset\")\n",
        "\n",
        "        train_dataset = CVSSDataset(train_encodings, train_labels)\n",
        "        test_dataset = CVSSDataset(test_encodings, test_labels)\n",
        "\n",
        "        # Training\n",
        "        print(\"### Training\")\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=train_batch_size,\n",
        "            per_device_eval_batch_size=test_batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            save_strategy=\"epoch\",\n",
        "            weight_decay=weight_decay,\n",
        "            warmup_steps=warmup_steps,\n",
        "            warmup_ratio=warmup_ratio,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        acc = infer(trainer, test_dataset, num_labels)\n",
        "        print(f\"Accuracy for {category['name']} = {acc:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
